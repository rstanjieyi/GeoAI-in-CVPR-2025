# <p align=center>`GeoAI in CVPR 2025`</p>

:star2:**A collection of papers related to Geo-spatial Information Science in CVPR 2025.**

## ğŸ“¢ Latest Updates
:fire::fire::fire: Last Updated on 2025.02.28 :fire::fire::fire:
- **2025.02.28**: Update 3 paper.


## :memo: CVPR 2025 Accepted Paper List


#### RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings

> Aayush Dhakal, Srikumar Sastry, Subash Khanal, Adeel Ahmad, Eric Xing, Nathan Jacobs

* Paper: https://arxiv.org/abs/2502.19781
* Dataset and code: None
* <details>
    <summary>Abstract (Click to expand):</summary>
    The choice of representation for geographic location significantly impacts the accuracy of models for a broad range of geospatial tasks, including fine-grained species classification, population density estimation, and biome classification. Recent works like SatCLIP and GeoCLIP learn such representations by contrastively aligning geolocation with co-located images. While these methods work exceptionally well, in this paper, we posit that the current training strategies fail to fully capture the important visual features. We provide an information theoretic perspective on why the resulting embeddings from these methods discard crucial visual information that is important for many downstream tasks. To solve this problem, we propose a novel retrieval-augmented strategy called RANGE. We build our method on the intuition that the visual features of a location can be estimated by combining the visual features from multiple similar-looking locations. We evaluate our method across a wide variety of tasks. Our results show that RANGE outperforms the existing state-of-the-art models with significant margins in most tasks. We show gains of up to 13.1% on classification tasks and 0.145 $R^2$ on regression tasks. All our code will be released on GitHub. Our models will be released on HuggingFace.
  </details>

#### SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images

> Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang

* Paper: https://arxiv.org/abs/2410.01768
* Project: https://likyoo.github.io/SegEarth-OV/
* <details>
    <summary>Abstract (Click to expand):</summary>
    Remote sensing image plays an irreplaceable role in fields such as agriculture, water resources, military, and disaster relief. Pixel-level interpretation is a critical aspect of remote sensing image applications; however, a prevalent limitation remains the need for extensive manual annotation. For this, we try to introduce open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle this issue, we propose a simple and general upsampler, SimFeatUp, to restore lost spatial information in deep features in a training-free style. Further, based on the observation of the abnormal response of local patch tokens to [CLS] token in CLIP, we propose to execute a straightforward subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets spanning semantic segmentation, building extraction, road detection, and flood detection tasks. Our method achieves an average of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art methods on 4 tasks. All codes are released.
  </details>

#### Towards Satellite Image Road Graph Extraction: A Global-Scale Dataset and A Novel Method

> Pan Yin, Kaiyu Li, Xiangyong Cao, Jing Yao, Lei Liu, Xueru Bai, Feng Zhou, Deyu Meng

* Paper: https://arxiv.org/abs/2411.16733
* Dataset and code: https://github.com/earth-insights/samroadplus
* <details>
    <summary>Abstract (Click to expand):</summary>
    Recently, road graph extraction has garnered increasing attention due to its crucial role in autonomous driving, navigation, etc. However, accurately and efficiently extracting road graphs remains a persistent challenge, primarily due to the severe scarcity of labeled data. To address this limitation, we collect a global-scale satellite road graph extraction dataset, i.e. Global-Scale dataset. Specifically, the Global-Scale dataset is âˆ¼20Ã— larger than the largest existing public road extraction dataset and spans over 13,800 km2 globally. Additionally, we develop a novel road graph extraction model, i.e. SAM-Road++, which adopts a node-guided resampling method to alleviate the mismatch issue between training and inference in SAM-Road, a pioneering state-of-the-art road graph extraction model. Furthermore, we propose a simple yet effective ``extended-line'' strategy in SAM-Road++ to mitigate the occlusion issue on the road. Extensive experiments demonstrate the validity of the collected Global-Scale dataset and the proposed SAM-Road++ method, particularly highlighting its superior predictive power in unseen regions. The dataset and code are available at https://github.com/earth-insights/samroadplus.
  </details>


#### RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark

> Xin Zhang, Xue Yang, Yuxuan Li, Jian Yang, Ming-Ming Cheng, Xiang Li

* Paper: https://arxiv.org/abs/2501.04440
* Dataset and code: https://github.com/zhasion/RSAR
* <details>
    <summary>Abstract (Click to expand):</summary>
    Rotated object detection has made significant progress in the optical remote sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are laggard behind, primarily due to the absence of a large-scale dataset. Annotating such a dataset is inefficient and costly. A promising solution is to employ a weakly supervised model (e.g., trained with available horizontal boxes only) to generate pseudo-rotated boxes for reference before manual calibration. Unfortunately, the existing weakly supervised models exhibit limited accuracy in predicting the object's angle. Previous works attempt to enhance angle prediction by using angle resolvers that decouple angles into cosine and sine encodings. In this work, we first reevaluate these resolvers from a unified perspective of dimension mapping and expose that they share the same shortcomings: these methods overlook the unit cycle constraint inherent in these encodings, easily leading to prediction biases. To address this issue, we propose the Unit Cycle Resolver, which incorporates a unit circle constraint loss to improve angle prediction accuracy. Our approach can effectively improve the performance of existing state-of-the-art weakly supervised methods and even surpasses fully supervised models on existing optical benchmarks (i.e., DOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce RSAR, the largest multi-class rotated SAR object detection dataset to date. Extensive experiments on both RSAR and optical datasets demonstrate that our UCR enhances angle prediction accuracy. Our dataset and code can be found at: https://github.com/zhasion/RSAR.
  </details>


#### AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation

> Datao Tang, Xiangyong Cao, Xuan Wu, Jialin Li, Jing Yao, Xueru Bai, Dongsheng Jiang, Yin Li, Deyu Meng

* Paper: https://arxiv.org/abs/2411.15497
* Dataset and code: https://github.com/Sonettoo/AeroGen
* <details>
    <summary>Abstract (Click to expand):</summary>
    Remote sensing image object detection (RSIOD) aims to identify and locate specific objects within satellite or aerial imagery. However, there is a scarcity of labeled data in current RSIOD datasets, which significantly limits the performance of current detection algorithms. Although existing techniques, e.g., data augmentation and semi-supervised learning, can mitigate this scarcity issue to some extent, they are heavily dependent on high-quality labeled data and perform worse in rare object classes. To address this issue, this paper proposes a layout-controllable diffusion generative model (i.e. AeroGen) tailored for RSIOD. To our knowledge, AeroGen is the first model to simultaneously support horizontal and rotated bounding box condition generation, thus enabling the generation of high-quality synthetic images that meet specific layout and object category requirements. Additionally, we propose an end-to-end data augmentation framework that integrates a diversity-conditioned generator and a filtering mechanism to enhance both the diversity and quality of generated data. Experimental results demonstrate that the synthetic data produced by our method are of high quality and diversity. Furthermore, the synthetic RSIOD data can significantly improve the detection performance of existing RSIOD models, i.e., the mAP metrics on DIOR, DIOR-R, and HRSC datasets are improved by 3.7%, 4.3%, and 2.43%, respectively. The code is available at https://github.com/Sonettoo/AeroGen.
  </details>

#### PEACE: Empowering Geologic Map Holistic Understanding with MLLMs

> Yangyu Huang, Tianyi Gao, Haoran Xu, Qihao Zhao, Yang Song, Zhipeng Gui, Tengchao Lv, Hao Chen, Lei Cui, Scarlett Li, Furu Wei

* Paper: https://arxiv.org/abs/2501.06184
* Dataset and code: https://github.com/microsoft/PEACE
* <details>
    <summary>Abstract (Click to expand):</summary>
    Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.
  </details>

#### Adaptive Rectangular Convolution for Remote Sensing Pansharpening

> Xueyang Wang, Zhixin Zheng, Jiandong Shao, Yule Duan, Liang-Jian Deng

* Paper: https://arxiv.org/abs/2503.00467v1
* Dataset and code: https://github.com/WangXueyang-uestc/ARConv
* <details>
    <summary>Abstract (Click to expand):</summary>
    Recent advancements in convolutional neural network (CNN)-based techniques for remote sensing pansharpening have markedly enhanced image quality. However, conventional convolutional modules in these methods have two critical drawbacks. First, the sampling positions in convolution operations are confined to a fixed square window. Second, the number of sampling points is preset and remains unchanged. Given the diverse object sizes in remote sensing images, these rigid parameters lead to suboptimal feature extraction. To overcome these limitations, we introduce an innovative convolutional module, Adaptive Rectangular Convolution (ARConv). ARConv adaptively learns both the height and width of the convolutional kernel and dynamically adjusts the number of sampling points based on the learned scale. This approach enables ARConv to effectively capture scale-specific features of various objects within an image, optimizing kernel sizes and sampling locations. Additionally, we propose ARNet, a network architecture in which ARConv is the primary convolutional module. Extensive evaluations across multiple datasets reveal the superiority of our method in enhancing pansharpening performance over previous techniques. Ablation studies and visualization further confirm the efficacy of ARConv.
  </details>
  
#### Cross-Rejective Open-Set SAR Image Registration

> æ¯›èèï¼Œè·¯ä¸–æ˜ï¼Œæœå¬é¾™ï¼Œç„¦ææˆï¼Œç¼‘æ°´å¹³ï¼Œç‰Ÿä¼¦ç”°ï¼Œé²å­¦æƒï¼Œç†Šéœ–ï¼Œå¼ è‰ºè’™

* Paper: None
* Dataset and code: None
* <details>
    <summary>Abstract (Click to expand):</summary>
    åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒé…å‡†æ˜¯é¥æ„Ÿç§‘å­¦åº”ç”¨ä¸­çš„ä¸€é¡¹å…³é”®ä¸Šæ¸¸ä»»åŠ¡ï¼Œé€šå¸¸ä»¥ä¸¤å¹…å›¾åƒé¢„æå–çš„å…³é”®ç‚¹ä½œä¸ºè§‚æµ‹å¯¹è±¡è¿›è¡ŒåŒ¹é…ç‚¹å¯¹æœç´¢ã€‚é€šå¸¸ï¼Œé…å‡†è¢«è§†ä¸ºä¸€ç§å…¸å‹çš„é—­é›†åˆ†ç±»é—®é¢˜ï¼Œå³å¼ºåˆ¶å°†æ¯ä¸ªå…³é”®ç‚¹å½’ç±»åˆ°å·²çŸ¥ç±»åˆ«ä¸­ï¼Œå´å¿½è§†äº†å¤§é‡å†—ä½™å…³é”®ç‚¹è¶…å‡ºé¢„è®¾ç±»åˆ«çš„æœ¬è´¨é—®é¢˜ï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´æ•æ‰åˆ°é”™è¯¯çš„åŒ¹é…ç‚¹å¯¹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨åŸŸæ‹’ç»å¼€é›†SARå›¾åƒé…å‡†æ–¹æ³•ï¼Œç®€ç§°CroR-OSIRã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå†—ä½™å…³é”®ç‚¹è¢«è§†ä¸ºåˆ†å¸ƒå¤–(OOD)æ ·æœ¬ï¼Œå°†é…å‡†é—®é¢˜é‡æ–°å®šä¹‰ä¸ºä¸€ç§ç‰¹æ®Šçš„å¼€é›†ä»»åŠ¡ã€‚è¯¥ç®—æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šç›‘ç£å¯¹æ¯”ç‰¹å¾æå–æ¨¡å—(SupCon)å’Œè·¨åŸŸæ‹’ç»å¼€é›†è¯†åˆ«æ¨¡å—(CroR-OSR)ã€‚åŒºåˆ«äºä¼ ç»Ÿå¼€æ”¾é›†è¯†åˆ«ï¼ŒCroR-OSRæ¨¡å—ä¸­æ‰€æœ‰æ ·æœ¬(åŒ…æ‹¬OODæ ·æœ¬)å‡å¯ç”¨äºè®­ç»ƒï¼Œå¹¶åœ¨ä¸¤å¹…å›¾åƒçš„ç‹¬ç«‹å¼€é›†åŸŸä¸­è¿›è¡Œé—­é›†åˆ†ç±»ï¼Œé€šè¿‡è®¾è®¡è·¨åŸŸæ‹’ç»æœºåˆ¶ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦å’Œä¸€è‡´æ€§å¯¹æ ·æœ¬ç‚¹è¿›è¡Œè¯„ä¼°ï¼Œæœ‰æ•ˆæ’é™¤éé…å‡†ç‚¹å¯¹çš„OODæ ·æœ¬ã€‚æ­¤å¤–ï¼Œå°†CroR-OSRæ¨¡å—è¾“å‡ºçš„è·¨åŸŸä¼°è®¡æ ‡ç­¾åé¦ˆè‡³SupConæ¨¡å—ï¼Œä»¥å¢å¼ºå…³é”®ç‚¹ç‰¹å¾çš„åˆ¤åˆ«æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é…å‡†ç²¾åº¦ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚
  </details>

#### Feature Spectrum Learning for Remote Sensing Change Detection

> è‡§çªï¼Œèµµæ ‹ï¼Œç‹çˆ½ï¼Œæƒè±†ï¼Œç„¦ææˆï¼Œé’Ÿå‡†
* Paper: None
* Dataset and code: None
* <details>
    <summary>Abstract (Click to expand):</summary>
    å˜åŒ–æ£€æµ‹ï¼ˆCDï¼‰å¯¹åœ°çƒè§‚æµ‹å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå…¶ä¸­ç”±æˆåƒç¯å¢ƒå› ç´ å¼•èµ·çš„åŒæ—¶ç›¸å›¾åƒä¹‹é—´çš„ä¼ªå˜åŒ–æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦å°†ä¼ªå˜åŒ–è§†ä¸ºä¸€ç§é£æ ¼è½¬ç§»ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å°†åŒæ—¶ç›¸å›¾åƒè½¬åŒ–ä¸ºç›¸åŒé£æ ¼æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„åŠªåŠ›å—åˆ°ä¼˜åŒ–GANså¤æ‚æ€§å’Œç¼ºä¹ç‰©ç†å±æ€§æŒ‡å¯¼çš„é™åˆ¶ã€‚æœ¬æ–‡å‘ç°ï¼Œè°±å˜æ¢ï¼ˆSTï¼‰æœ‰æ½œåŠ›é€šè¿‡åœ¨é¢‘åŸŸä¸­å¯¹é½æ‰¿è½½é£æ ¼çš„ä¿¡æ¯æ¥å‡è½»ä¼ªå˜åŒ–ã€‚ç„¶è€Œï¼ŒSTçš„ä¼˜åŠ¿åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°ä¸¤ä¸ªç¼ºç‚¹çš„åˆ¶çº¦ï¼š1ï¼‰æœ‰é™çš„å˜æ¢ç©ºé—´å’Œ2ï¼‰ä½æ•ˆçš„å‚æ•°æœç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ç‰¹å¾è°±å­¦ä¹ ï¼ˆFeaSpectï¼‰ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è‡ªé€‚åº”åœ°æ¶ˆé™¤ä¼ªå˜åŒ–ã€‚é’ˆå¯¹ç¼ºç‚¹1ï¼‰ï¼ŒFeaSpecté€šè¿‡ç‰¹å¾è°±å˜æ¢ï¼ˆFSTï¼‰å¼•å¯¼å˜æ¢èµ°å‘é£æ ¼å¯¹é½çš„åˆ¤åˆ«æ€§ç‰¹å¾ã€‚é’ˆå¯¹ç¼ºç‚¹2ï¼‰ï¼ŒFeaSpectä½¿å¾—FSTå¯ä»¥è®­ç»ƒï¼Œä»è€Œé€šè¿‡è‡ªé€‚åº”æ³¨æ„åŠ›æå–æ¡†å’Œå¯å­¦ä¹ æ­¥å¹…æå–æ¡†é«˜æ•ˆåœ°å‘ç°æœ€ä¼˜å‚æ•°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†è‰¯å¥½çš„æƒè¡¡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾åœ°åµŒå…¥åˆ°å…¶ä»–æ¡†æ¶ä¸­ï¼Œè·å¾—ä¸€è‡´çš„æå‡ã€‚
  </details>

#### Towards Generalizable Scene Change Detection

> Jaewoo Kim, Uehwan Kim
* Paper: https://arxiv.org/abs/2409.06214
* Dataset and code: None
* <details>
    <summary>Abstract (Click to expand):</summary>
    While current state-of-the-art Scene Change Detection (SCD) approaches achieve impressive results in well-trained research data, they become unreliable under unseen environments and different temporal conditions; in-domain performance drops from 77.6% to 8.0% in a previously unseen environment and to 4.6% under a different temporal condition -- calling for generalizable SCD and benchmark. In this work, we propose the Generalizable Scene Change Detection Framework (GeSCF), which addresses unseen domain performance and temporal consistency -- to meet the growing demand for anything SCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a zero-shot manner. For this, we design Initial Pseudo-mask Generation and Geometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and single-image based segmentation into scene change detection for a pair of inputs without guidance. Furthermore, we define the Generalizable Scene Change Detection (GeSCD) benchmark along with novel metrics and an evaluation protocol to facilitate SCD research in generalizability. In the process, we introduce the ChangeVPR dataset, a collection of challenging image pairs with diverse environmental scenarios -- including urban, suburban, and rural settings. Extensive experiments across various datasets demonstrate that GeSCF achieves an average performance gain of 19.2% on existing SCD datasets and 30.0% on the ChangeVPR dataset, nearly doubling the prior art performance. We believe our work can lay a solid foundation for robust and generalizable SCD research.
  </details>

